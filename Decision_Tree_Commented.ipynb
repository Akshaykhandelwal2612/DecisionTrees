{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "Current Entropy is= 1.5849625007211559\n",
      "Splitting on feature pw_labeled with gain ratio 0.6996382036222091\n",
      "Level  1\n",
      "Current Entropy is= 0.8631205685666303\n",
      "Splitting on feature pl_labeled with gain ratio 0.4334099495621061\n",
      "Level  2\n",
      "Current Entropy is= 0.6581912658132184\n",
      "Splitting on feature sl_labeled with gain ratio 0.12674503775809312\n",
      "Level  3\n",
      "Current Entropy is= 0.7837769474847011\n",
      "Splitting on feature sw_labeled with gain ratio 0.07092036405148876\n",
      "Level  4\n",
      "No more features to split on \n",
      "Level  4\n",
      "No more features to split on \n",
      "Level  4\n",
      "No more features to split on \n",
      "Level  3\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n",
      "Level  3\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n",
      "Level  3\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n",
      "Level  2\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n",
      "Level  2\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n",
      "Level  1\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n",
      "Level  1\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n",
      "Level  1\n",
      "Current Entropy is= 0.0\n",
      "Reached Leaf Node\n"
     ]
    }
   ],
   "source": [
    "#import all the required libraries like pandas numpy etc that will be required throughout the code \n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#the function that convert the continiuos dataset into classes so that we can implement and make decision tree from it. Here we categorise the entire dataset into 4 categories a,b,c,d respectively\n",
    "def label(val, *boundaries):\n",
    "    if (val < boundaries[0]):\n",
    "        return 'a'\n",
    "    elif (val < boundaries[1]):\n",
    "        return 'b'\n",
    "    elif (val < boundaries[2]):\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "#the function to find the values on basis of which we will divide the categories and give call to label function so that the value could be assigned a label here we divide the entire dataset based on the mean values and their average with min and max values \n",
    "def toLabel(df, old_feature_name):\n",
    "    second = df[old_feature_name].mean()\n",
    "    minimum = df[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = df[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    #now we will make a call to the label function and pass the values calculated as the arguments \n",
    "    return df[old_feature_name].apply(label, args= (first, second, third))\n",
    "\n",
    "#info function is used to calculate the  entropy \n",
    "def info(y):\n",
    "        # d is the total number of values that is the denominator part and then we calculate the entropy using the  formula and return the final value                               \n",
    "        d=len(y)\n",
    "        val=0\n",
    "        x=y.value_counts()\n",
    "        for i in x:\n",
    "            val-=((i/d)*(math.log2(i)-math.log2(d)))\n",
    "        return val  \n",
    "\n",
    "#This function recieves the input data and results along with the feature list and levels as an input and then makes the final structure            \n",
    "def build_tree(df, y, unused_features,lev=0):\n",
    "    #we will check on our base case 1 which is in case we are left with no other features to split upon and thus we have to return from there \n",
    "    if len(unused_features)==0:\n",
    "        print(\"Level \",lev)\n",
    "        print(\"No more features to split on \")\n",
    "        return \n",
    "    \n",
    "    best_feature = \"\"\n",
    "    #Now we will lookout for the entropy value by calling the info function  \n",
    "    y_info=info(y)\n",
    "    gain_ratio=0\n",
    "    \n",
    "    #check the value of the results in case we have Entropy(y_info) as 0 that means all the data we have is pure and cant be filtered ahead and thus we return from there as we reached the leaf node \n",
    "    if y_info==0 :\n",
    "        \n",
    "        print(\"Level \",lev)\n",
    "        print(\"Current Entropy is=\",y_info)\n",
    "        print(\"Reached Leaf Node\")\n",
    "        return \n",
    "    \n",
    "    #now we will start iterating over the unused features \n",
    "    for f in unused_features:\n",
    "        # now for each feature we find list of possible values and then iterate over each value \n",
    "        possible_values = set(df[f])\n",
    "        inf=0\n",
    "        split_info=0\n",
    "        for j in possible_values:\n",
    "            #now we will find the entropy and split info and will calculate the information gain as well the denominator will be total number of values \n",
    "            a=df[f]\n",
    "            den=len(a)\n",
    "            \n",
    "            true_vals=(a==j)\n",
    "            t=a[true_vals]\n",
    "            #the numerator will be the number of entries with that particualr value \n",
    "            num=len(t)\n",
    "            y2=y[true_vals]\n",
    "            \n",
    "            y2_info=info(y2)\n",
    "            \n",
    "            #now we will do the weighted addition of the entropy for the individual splits \n",
    "            inf+=((num/den)*y2_info)\n",
    "            #we will also calculate the split info using the formula \n",
    "            split_info-=((num/den)*(math.log2(num)-math.log2(den)))\n",
    "        #now we will calulate the gain ratio and will compare it with the present value if its greater then update the values to the greater value of gain ratio and change the best feature of that to the feature whose value we are storing in gain ratio\n",
    "        gain=((y_info-inf)/split_info)\n",
    "        if gain>gain_ratio:\n",
    "            gain_ratio=gain\n",
    "            best_feature=f\n",
    "            \n",
    "    #print the required information \n",
    "    print(\"Level \",lev)\n",
    "    print(\"Current Entropy is=\",y_info)\n",
    "    print(\"Splitting on feature\",best_feature,\"with gain ratio\",gain_ratio)\n",
    "    possible_values=set(df[best_feature])\n",
    "    \n",
    "    #now split the dataset based on the possible values of the best feature and then start sending the split data to build tree function for further analysis of data \n",
    "    for i in possible_values:\n",
    "        x2=df[df[best_feature]==i]\n",
    "        y2=y[df[best_feature]==i]\n",
    "        #here we will filter data and before sending the data we will reomve the current feature from best feature so that data doesnt split on same feature twice \n",
    "        unused_features.remove(best_feature)\n",
    "        build_tree(x2,y2,unused_features,lev+1)\n",
    "        unused_features.add(best_feature)\n",
    "    #once everythinng is done we will return \n",
    "    return\n",
    "\n",
    "#load the iris dataset from the datasets and convert that into a pandas dataframe                                      \n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data)\n",
    "#name the coulums \n",
    "df.columns = [\"sl\", \"sw\", 'pl', 'pw']\n",
    "#now convert the continious data to the classified/labeled data by making a call to toLabel function \n",
    "df['sl_labeled'] = toLabel(df, 'sl')\n",
    "df['sw_labeled'] = toLabel(df, 'sw')\n",
    "df['pl_labeled'] = toLabel(df, 'pl')\n",
    "df['pw_labeled'] = toLabel(df, 'pw')\n",
    "#now drop the coulums with continious data \n",
    "df.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)\n",
    "#load the final results from the target frame \n",
    "y = pd.DataFrame(iris.target)\n",
    "#now store all the unused parameters in the set and pass all the required values to the build tree function \n",
    "unused_features = set(df.columns)\n",
    "\n",
    "build_tree(df, y, unused_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
